{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d521723-561f-4baa-b71c-7605e1fb7e5d",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb20a47-40c2-489e-8425-8dcc3740df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import joblib\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa9547-ab29-4884-b4e4-59d578e01061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3759f4e5-016f-40e3-aa5c-89ec298eb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    X = df.drop(columns=['is_high_risk', 'CustomerId'])\n",
    "    y = df['is_high_risk']\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad583a92-ca42-42e7-9e71-77cc1fbae5aa",
   "metadata": {},
   "source": [
    "# Split the input DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "301825a0-52e9-41be-b53f-f33c4ea61d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_column='is_high_risk'):\n",
    "\n",
    "    drop_cols = [\n",
    "        'TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId',\n",
    "        'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory',\n",
    "        'ChannelId', 'TransactionStartTime'  # drop datetime for now\n",
    "    ]\n",
    "    X = df.drop(columns=[target_column] + drop_cols, errors='ignore')\n",
    "    y = df[target_column]\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12237099-0568-4a70-8fdc-54674e4c6e7b",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdb9e40f-a1ae-4cad-aff7-bd86cfb5e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train):\n",
    "    param_grid = {'C': [0.1, 1.0, 10.0]}\n",
    "    grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='f1')\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a711936-3508-47ce-9eb3-7de18b3867e4",
   "metadata": {},
   "source": [
    "# Train a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57a31cf-5bc1-4b98-b8b2-a4c5894a7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train):\n",
    "    param_grid = {'n_estimators': [50, 100], 'max_depth': [5, 10, None]}\n",
    "    grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1')\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981c5f7-be1b-4cd8-9f5b-9511d86814fe",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8870bb15-7672-4552-bf89-5fef54d1bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786fe0f-8e60-4291-85cd-b877f482f08c",
   "metadata": {},
   "source": [
    "# train and log model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fab4f45-3c61-4ab9-a641-9ba7493f58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(name, model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        grid = GridSearchCV(model, param_grid, cv=3, scoring='f1')\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_test, y_pred, y_proba)\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            mlflow.log_metric(k, v)\n",
    "\n",
    "        mlflow.sklearn.log_model(best_model, name)\n",
    "\n",
    "        model_path = f\"./models/{name}.pkl\"\n",
    "        joblib.dump(best_model, model_path)\n",
    "        print(f\"Saved model to {model_path}\")\n",
    "\n",
    "        return best_model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7058b88e-eed7-40e4-accf-64c71428b2aa",
   "metadata": {},
   "source": [
    "# load dataset from CSV, train both models, evaluate, and log with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15072dc2-fb10-4a20-ab2a-1923cb6f1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    #mlflow.set_experiment(\"Credit Risk Modeling\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        models = {\n",
    "            'LogisticRegression': train_logistic_regression(X_train, y_train),\n",
    "            'RandomForest': train_random_forest(X_train, y_train)\n",
    "        }\n",
    "\n",
    "        best_model = None\n",
    "        best_score = 0\n",
    "\n",
    "        for name, model in models.items():\n",
    "            metrics = evaluate_model(model, X_test, y_test)\n",
    "            mlflow.log_params(model.get_params())\n",
    "            mlflow.log_metrics(metrics)\n",
    "            mlflow.sklearn.log_model(model, name)\n",
    "\n",
    "            if metrics['f1'] > best_score:\n",
    "                best_score = metrics['f1']\n",
    "                best_model = model\n",
    "\n",
    "        # Save the best model locally\n",
    "        joblib.dump(best_model, '../data/models/best_model.pkl')\n",
    "\n",
    "        # Register the best model to MLflow\n",
    "        mlflow.sklearn.log_model(best_model, \"model\", registered_model_name=\"CreditRiskBestModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be42df-4def-4a4f-bc41-df1205829eb6",
   "metadata": {},
   "source": [
    "# Run training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eabf97c1-ecea-44fc-8fdb-9f5dee0584c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\miniconda4\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "2025/07/17 15:01:43 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/17 15:01:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 15:01:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/17 15:02:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/17 15:02:09 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/17 15:02:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'CreditRiskBestModel'.\n",
      "Created version '1' of model 'CreditRiskBestModel'.\n"
     ]
    }
   ],
   "source": [
    "# Ensure directory exists\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "run_training_pipeline(\"../data/labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a4387-c130-43ae-9c08-2b5aa5f4e10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
